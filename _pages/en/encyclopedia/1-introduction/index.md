---
nav_order: 1
has_children: true
permalink: /encyclopedia
title: Introduction
numbered_headers: false
css: home
---

## If you’re reading this, you’re probably trying to counter disinformation.

## ![](/assets/img/home/not-alone.png){: #not-alone-illustration } Good news!<br />You’re not alone.
{: .mt-0 .thin }

- - -

#### ![](/assets/img/home/criteria.png){: #criteria-illustration } To make it easier to share and collaborate with others who share your aim, you first have to determine the **criteria** that guide your action.
{: .thin}

There is no unique definition for information manipulation and what kind warrants a reaction. Each organization has its own mandate and along with it, specific criteria to distinguish “legit” information from disinformation.

![](/assets/img/home/subjectivity.png){: #subjectivity-illustration } For instance, a Foreign Affairs Ministry will deem crucial the criterion of “foreign source”. Or, a europhile association will only target eurosceptic content, making europhobia a decisive criteria, whereas a nationalist group will focus on fact-checking EU-supportive content.

![](/assets/img/home/convergence.png){: #convergence-illustration } Spelling out your criteria allows you to mutualise means with other actors while preserving their mandate… and your own, by making it explicit!


#### The intersection of each actor’s criteria defines what disinformation is to them. ![](/assets/img/home/disinformation.png){: #disinformation-illustration }
{: .emphasis}

- - -

#### Now that we know how to differentiate legitimate content from manipulated information, let’s clarify the process through which actors go to counter it.
{: .thin}

### Detection
#### First, you need to [detect](/encyclopedia/detection) suspicious content on the Internet.
{: .thin}

![](/assets/img/home/detection.png){: #detection-illustration } This can be done with [human intelligence](/encyclopedia/detection/practices), or more likely with [software](/encyclopedia/detection/tools) that scans through specific sources to highlight suspicious content. At this stage, what you want is only to identify which content deserves investing more resources.

### Qualification
#### Indeed, human intervention is necessary to [qualify](/encyclopedia/qualification) content according to your criteria.
{: .thin}

![](/assets/img/home/qualification.png){: #qualification-illustration } You need to strike a balance between [obtaining](/encyclopedia/qualification/tools) enough certainty that a possible reaction is appropriate, and being fast enough for it to be impactful.
Very often, what was detected as suspicious does not qualify as actually harmful, hence the importance of this phase.

### Reaction
#### For these pieces of content that actually qualify as disinformation, your organisation will have some [reaction](/encyclopedia/reaction) mechanism to activate.
{: .thin}

![](/assets/img/home/reaction.png){: #reaction-illustration } It might be issuing a [public rebuttal](/encyclopedia/reaction/practices#denial), a [takedown](/encyclopedia/reaction/practices#content-takedown) notice, exposing the mischief, activating [elves](/encyclopedia/reaction/practices#elves) networks… or simply doing nothing because the risk of backfire always exists!

### Attribution
#### Another resource consuming activity is [attributing](/encyclopedia/attribution) the origin of disinformation to specific agents.
{: .thin}

![](/assets/img/home/attribution.png){: #attribution-illustration }
While useful to inform the best course of action, this phase is not needed to react. Besides, attribution does not share the urgency that an impactful reaction depends on. Related [practices](/encyclopedia/attribution/case-studies) and [tools](/encyclopedia/attribution/tools) are part of the forensics and open-source intelligence (OSINT) fields. Attribution brings the most value by linking together seemingly independent campaigns, allowing to refine detection and reaction means through the unveiling of patterns.

### Research
#### Going beyond specific campaigns, [research](/encyclopedia/research) adds to the knowledge of information manipulation.
{: .thin}

![](/assets/img/home/research.png){: #research-illustration } [Studying](/encyclopedia/research/practices) past disinformation efforts allows to more easily identify and assess future ones. It can also be used to [evaluate](/encyclopedia/research/tools) the effectiveness of tools and practices that counter them. Information manipulation has no dedicated academic field, most of the [current research](/encyclopedia/research/actors) on the topic happens at the intersection of social and computer sciences.

### Prevention
#### [Prevention](/encyclopedia/prevention) raises awareness and increases media literacy and societal cohesion.
{: .thin}

![](/assets/img/home/prevention.png){: #prevention-illustration } Disinformation targets human societies. While research can help us understand how attacks work, and the detection-qualification-reaction sequence can defuse many of them, the most viable [long-term solution](/encyclopedia/prevention/tools) is to immunize the entire social body.
{: .fix-height-with-floated-element }

- - -

### Foster collaboration and increase impact
#### This collaborative resource aims at empowering all actors countering information manipulation to grow and improve.
{: .thin}

To do so, it unifies vocabulary and provides concepts like the ones you just read. It also gathers exemplary, sharable [case studies](/encyclopedia/reaction/case-studies) and describes the best [practices](/encyclopedia/reaction/practices), [tools](/encyclopedia/qualification/tools) and [actors](/encyclopedia/research/actors) in the field. Furthermore, it consolidates opportunities such as [funding](/encyclopedia/funders).

Your [contributions](/encyclopedia/contribute) are more than welcome.
