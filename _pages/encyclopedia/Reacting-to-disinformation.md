---
title: Reacting-to-disinformation
permalink: /encyclopedia/reacting-to-disinformation/
nav_order: 10
---

# Reacting to disinformation

Ways to react to and further counter disinformation are as various as the actors involved in fighting it. Judiciary measures, cooperative fact-checking initiatives, political statements, civil society spontaneous organizations are only a few examples of the many techniques developed in reaction to disinformation campaigns.


* [1. NewsGuard Tech](#newsguard-technologies)
* [2. Elves](#elves)
* [3. Correct the record](#correct-the-record)
* [4. Judiciary](#judiciary)
* [5. Denial](#denial)
* [6. 3PFCT](#facebooks-third-party-fact-checking-tool)


## NewsGuard Technologies

NewsGuard Technologies has developed a free browser extension, NewsGuard, which rates websites according to their transparency and credibility.

Journalists, editors and analysts at NewsGuard rate and review when possible "_thousands of news and information websites_" based on the repeated publication of false content, the responsibility of the gathering and presentation of information, error correction or clarification, the differentiation between news and opinion, the deceptiveness of headlines, the disclosure of ownership and financing, the clarity of ad labels, the level of information about content supervisors as well as the level of information given about content providers.

NewsGuard classifies websites in either **platforms**, **satires**, **reds** or **greens**.

Platforms are sites that are hard to define according to transparency or credibility criteria. They primarily host non-vetted user-generated content but their reliability is difficult to assess. As to satires, they are not "real news websites" but may have a humoristic purpose, for instance.

Green websites are deemed credible and reliable. Detail about which criteria are filled can be provided by NewsGuard, thus adding nuance.

Red websites simply fail "_to meet basic standards of credibility and transparency_".

Ratings are periodically updated. Before being rated, suspicious websites are directly called by NewsGuard to obtain more information. For each site, a "Nutrition Label" is drafted by analysts. It contains "_the site’s performance on each of the nine criteria and a written explanation of the content on the site, who’s behind it, and why it received its rating_".

## Elves

[Elves](https://desinfo.quaidorsay.fr/encyclopedia/definitions/vocabulary/#elves) is a term born in the Baltic states (Lithuania, Latvia and Estonia), which later spread south in eastern and later western Europe. Hunting and fighting trolls, they usually work in self-organized networks to counter disinformation. Their activities include debunking, the spreading of fact-checks, and the finding of fake accounts - among others.

It is difficult to assess the level of influence exerted by their actions. However, especially in the Baltic states and more particularly Lithuania, elves have gained significant media attention in their digital warfare against pro-Kremlin trolls.

## Correct the record

On March 12, 2019, international cyberactivism NGO Avaaz ("voice" in sanskrit) published a report called "Yellow Vests flooded by Fake News - Over 100M views if disinformation on Facebook" in which it called on Facebook to "**Correct the Record**" ahead of EU Elections.
Alongside a study on disinformation - especially Russian campaigns - focused on the Yellow Vests movement in France, Avaaz details its proposal for an innovative solution.

Based on cooperation between factcheckers and platforms (especially Facebook), the initiative works following a [five-step process](https://g8fip1kplyr33r3krz5b97d1-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/AVAAZ_YellowVests_100miofake.pdf.pdf.pdf).
First,  content **viewed by a significant number of people** and deemed false or misleading after **verification by independent fact-checkers** would "activate" an obligation for platforms to correct the record.
Then, platforms woud have to provide a **misinformation report mechanism** that is easy to access for users, as well as access for fact-checkers to content that has been viewed by a significant number of people.
Reported content should then be **fact-checked** within 24 hours by "_independent, third-party, verified factcheckers_" working with the platforms.
Platforms are also asked to display their "_most visible notification standard_" to **notify** all users exposed to verified
disinformation.
Lastly,  "_each user exposed to disinformation should receive a correction that is of at least equal prominence to the original content and that follows best practices_", specifically adapted to the user's profile.

Ultimately, all platforms users exposed to disinformation would receive independent third party corrections. One key-point is the avoidance of the repetition of disinformation.

Through this initiative, Avaaz's goal is to "_restore the public’s trust_" and "_ensure the integrity_" of the upcoming European elections.

More information as to the usefulness and effectiveness of correction in hindering the effects of disinformation can be found [here](https://g8fip1kplyr33r3krz5b97d1-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/AVAAZ_YellowVests_100miofake.pdf.pdf.pdf) (p.14/28), [here](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/112200/jcom12164.pdf?sequence=1&isAllowed=y) and [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5383823/).

## Judiciary

Some countries, like Switzerland, have decided that the fight against disinformation was not a priority. However, several states have taken measures and implemented laws to counter the spread and effects of disinformation on their soil.

⦁	In France

A law going back to July 29, 1881 (revised since) regarding freedom of the press already condemns the "_publication, diffusion or reproduction, by any means, of fake news, pieces that are fabricated, falisfied or falsely attributed to third parties when, made in bad faith, it disrupted public peace or could have done so_" to a 45 000 euros fine.

⦁	In Germany

The NetzDG law (Netzwerkdurchsetzungsgesetz, literally "_law on implementation / enforcement for networks_") passed on January 1st, 2018. It aims at fighting hate speech and disinformation on social networks. Platforms are obligated to remove such content once it has been reported. They have from 24hours up to a week to do so, depending on the obviousness of the falsehood or of the illicit character of the content.

If they do not comply, platforms must pay a fine of maximum 50 million euros, with possible individual fines of up to 5 millions euros for platforms leaders.

⦁	In Kenya

On May 16, 2018, a law against cybercrime and cyberbullying was passed, which condemns - among seventeen cybercrimes - the publication of "_false, misleading or fictitious data_" to a fine of 42 000 euros and/or a two-year prison sentence.

⦁	In Indonesia

Since January 2018, webizens who publish fake news face a prison sentence of up to six years.

⦁	In Italy

Through a website, citizens can report potentially false information to the police. A specialized police force for communication is in charge of verifying or fact-checking reported news, and must publish a statement of denial in case it is proven false. It is then possible for this police force to bring the case to judiciary authorities.

⦁	In Russia

In March 2019, two laws were passed to fight disinformation on the Internet and especially social media.

The first one is aimed at "socially significant false information that are spread as true information" and constitute a "threat to public or state security" or may cause "massive disruptions".

Platforms and online media outlets which spread "fake news" can be fined for up to 20 500 euros (1.5 million of roubles). If a media refuses to suppress what is deemed to be false information, its activity can be suspended.

However, no clear definition of what constitutes a "fake news" is given, as definition is left to the appreciation of the prosecutor, case-by-case.

The second one punishes "disrespect" toward state authorities and "offenses to state symbols" of fines and up to fifteen days of imprisonment in case of recidivism.

## Denial

Quickly denying one's involvement or responsibility in an event or domain can be a way for an actor to counter the effects of the spread of disinformation. Given that in interference occurrences, disinformation campaigns often aim at weakening or destabilizing powerful and / or state instances, a direct denial reaction can be appropriate.

For instance, in March 2019, several Algerian media outlets (including _observalgerie.com_ and _algeriepatriotique.com_ ) spread the fake news that France was suspending visa delivery for Algerian natives. The French Consulate in Algeria publicly reacted the same day.

It is difficult to assess the efficiency of such denials but direct reactions can help discredit disinformation-spreading outlets, instances and individuals as well as facilitate the work and action of fact-checkers.

## Facebooks' Third-Party Fact Checking Tool

The Third-Party Fact Checking Tool is a Facebook fact-checking program developed from December 2016 onwards.
Facebook already used methods to fight disinformation, such as the removal of accounts or content that violate its policies or the provision of context to users for specific stories.

It developed several partnerships with **third-party fact-checking organizations**, which help Facebook in the identification of false stories. As of April 2019, Facebook had **25 partners** (including the Agence France-Presse (AFP), India Today Fact Check, Africa Check Kenya, Faktisk (Norway), Teyit (Turkey) and Factcheck.org (US) ) in **27 countries** (Argentina, Brazil, Cameroon, Canada, Columbia, Danemark, France, Germany, India, Indonesia, Israël, Italy, Kenya, Mexico, the Netherlands, Nigeria, Norway, Pakistan, the Philippines, Poland, Senegal, South Africa, Spain, Sweden, Turkay, the UK and the US).

These fact-checkers are independent and certified through the non-partisan International Fact-Checking Network (IFCN). They can review **public or media content including links to articles, photos and videos**.

⦁	Detection

Detection relies heavily on users. It can either be based on **reports** ( _i.e._ when Facebook users submit feedback on a story,
claiming that it is false) or on **comments** (_i.e._ "_when users comment on an article expressing disbelief_").

In the US, machine-learning is developed by using already fact-checked and reviewed articles.
Detection can also be **proactive**, that is to say fact-checkers can identify stories to rate before being notified.

⦁	Analysis

Fact-checkers review and rate stories, then provide reference articles detailing their fact-check and reasons for the rating. Articles are shown in a "_Related Articles_" section just below the story reviewed in a user's news feeds.

The rating offers on 9 options:

- False: when the content is factually false
- Mix: when it is a mix of exact and false information / when the main claim is false or incomplete
- Misleading title: when the main claim of the article is factually true, but not the main claim of its title
- True: when the content is factually true
- Ineligible: when content can't be verified, or when claims were ture when they were redacted, or when content is coming from a site or webpage that specifically diffuses the opinions or the program of a political personnality
- Satire : when the content is explicitely enough satirical
- Opinion
- Hoax generator: when a website allows users to create their own hoaxes and share them via social media platforms
- Not evaluated : when the content has not been reviewed yet or cannot be evaluated. It can also be used when fact-checkers deem that no measure ought to be implemented.

⦁	Reaction

**Fake accounts**, tools often used to spread disinformation, can be removed.

Once reviewed, **false content** is **demoted** and it is shown lower in the news feed, which means that  its **visibility** is reduced - by over 80% on average.
Facebook also provides **more context** to its users. People who try to share a false story as well as previous sharers are notified of "_more reporting on the subject_".

A Facebook page or a website that repeatedly shares misinformation are sanctioned: their visibility is reduced and they can **no longer make money or advertise** on Facebook.

It is possible for editors and publishers to challenge a decision, in which case they can directly contact fact-checkers "_to dispute their rating or offer a correction_".
